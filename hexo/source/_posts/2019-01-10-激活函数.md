---
layout:     post                    # 使用的布局（不需要改）
title:      激活函数                # 标题 
date:       2019-01-10              # 时间
categories: Machine Learning                      
tags:                               #标签
    - Machine Learning 
    - Deep Learning
---

#### 什么是激活函数？

> 在神经网络的输出后，或者在两层网络之间，使用的一种转换函数

#### 神经网络中为什么要使用激活函数？

> 将网络的输出映射到固定的范围，如0~1或者-1~1等。

#### 激活函数分类：

* 线性激活函数
* 非线性激活函数
  * Sigmoid
  * tanh
  * Softmax
  * ReLU
  * Leaky ReLU

##### 1. 线性激活函数

![](https://res.cloudinary.com/chenzhen/image/upload/v1547557056/github_image/2019-01-10/2019-01-10-Linear.jpg)

​	线性的激活函数，数据的映射范围在正无穷到负无穷，具体取决于实际数据的范围。



##### 2. 非线性激活函数：Sigmoid

![](https://res.cloudinary.com/chenzhen/image/upload/v1547557056/github_image/2019-01-10/2019-01-10-Sigmoid.jpg)

​	将数据映射在0~1范围内

##### 3.非线性激活函数：Softmax

​	将全部数据映射到0~1范围，同时，映射后数据的总和为1，常用于多分类最后概率的输出

##### 4. 非线性激活函数：tanh

![](https://res.cloudinary.com/chenzhen/image/upload/v1547557056/github_image/2019-01-10/2019-01-10-tanh.jpg)

​	将数据映射在-1~1范围内

##### 5. 非线性激活函数：ReLU

![](https://res.cloudinary.com/chenzhen/image/upload/v1547557056/github_image/2019-01-10/2019-01-10-ReLU.jpg)

​	将数据中所有小于0的映射成0，大于0的数据扔是其本身

##### 6. 非线性激活函数：Leaky ReLU

![](https://res.cloudinary.com/chenzhen/image/upload/v1547557056/github_image/2019-01-10/2019-01-10-Leaky_ReLU.jpg)

​	类似于ReLU函数，当数据大于0时，和ReLU激活函数一致，仍是其数据本身；当数据小于0，会乘一个小数，这样的话，不会完全的将小于0的数值都置为0.